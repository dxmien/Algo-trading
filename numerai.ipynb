{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numerapi import NumerAPI\n",
    "import dotenv, os, json, pandas as pd, lightgbm as lgb\n",
    "\n",
    "dotenv.load_dotenv('./.env')\n",
    "\n",
    "P_ID = os.getenv('NUMERAI_PUBLIC_ID')\n",
    "S_ID = os.getenv('NUMERAI_SECRET_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_sets 17\n",
      "targets 37\n",
      "small 42\n",
      "medium 705\n",
      "all 2376\n"
     ]
    }
   ],
   "source": [
    "# Initialize numerapi\n",
    "napi = NumerAPI(public_id = P_ID, secret_key = S_ID)\n",
    "\n",
    "all_datasets = napi.list_datasets()\n",
    "#print(all_datasets)\n",
    "\n",
    "# Set data version to one of the latest datasets\n",
    "DATA_VERSION = \"v5.0\"\n",
    "\n",
    "# download the feature metadata file\n",
    "if not os.path.exists(f\"{DATA_VERSION}/features.json\"):\n",
    "    napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
    "\n",
    "# read the metadata and display\n",
    "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\")) # all the feature sets and targets (the names)\n",
    "#print(json.dumps(feature_metadata, indent=2))\n",
    "for metadata in feature_metadata:\n",
    "  print(metadata, len(feature_metadata[metadata]))\n",
    "\n",
    "# display the feature sets\n",
    "feature_sets = feature_metadata[\"feature_sets\"]\n",
    "#print(feature_sets[\"small\"])\n",
    "for feature_set in [\"small\", \"medium\", \"all\"]:\n",
    "  print(feature_set, len(feature_sets[feature_set]))\n",
    "\n",
    "# Only work with the medium feature set\n",
    "medium_feature_set = feature_sets[\"medium\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the training data \n",
    "if not os.path.exists(f\"{DATA_VERSION}/train.parquet\"):  \n",
    "  napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
    "\n",
    "# Load only the \"medium\" feature set \n",
    "train_feature_set = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/train.parquet\",\n",
    "    columns = [\"era\", \"target\"] + medium_feature_set\n",
    ")\n",
    "#print(train_feature_set.head())\n",
    "\n",
    "# Downsample to every 4th era to reduce memory usage and speedup model training\n",
    "train_feature_set_reduced = train_feature_set[train_feature_set[\"era\"].isin(train_feature_set[\"era\"].unique()[::4])]\n",
    "#print(train_feature_set_reduced.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "model = lgb.LGBMRegressor(\n",
    "  n_estimators=2000,\n",
    "  learning_rate=0.01,\n",
    "  max_depth=5,\n",
    "  num_leaves=2**5-1,\n",
    "  colsample_bytree=0.1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "  train_feature_set_reduced[medium_feature_set],\n",
    "  train_feature_set_reduced[\"target\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download validation data - this will take a few minutes\n",
    "napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
    "\n",
    "# Load the validation data and filter for data_type == \"validation\"\n",
    "validation = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/validation.parquet\",\n",
    "    columns=[\"era\", \"data_type\", \"target\"] + feature_set\n",
    ")\n",
    "validation = validation[validation[\"data_type\"] == \"validation\"]\n",
    "del validation[\"data_type\"]\n",
    "\n",
    "# Downsample to every 4th era to reduce memory usage and speedup evaluation (suggested for Colab free tier)\n",
    "# Comment out the line below to use all the data (slower and higher memory usage, but more accurate evaluation)\n",
    "validation = validation[validation[\"era\"].isin(validation[\"era\"].unique()[::4])]\n",
    "\n",
    "# Eras are 1 week apart, but targets look 20 days (o 4 weeks/eras) into the future,\n",
    "# so we need to \"embargo\" the first 4 eras following our last train era to avoid \"data leakage\"\n",
    "last_train_era = int(train[\"era\"].unique()[-1])\n",
    "eras_to_embargo = [str(era).zfill(4) for era in [last_train_era + i for i in range(4)]]\n",
    "validation = validation[~validation[\"era\"].isin(eras_to_embargo)]\n",
    "\n",
    "# Generate predictions against the out-of-sample validation features\n",
    "# This will take a few minutes üçµ\n",
    "validation[\"prediction\"] = model.predict(validation[feature_set])\n",
    "validation[[\"era\", \"prediction\", \"target\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vvv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
